{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------\n",
      "Enter a restaurant review (Type 'Q' to Quit):\n",
      "waiter was rude but pasta was nice\n",
      "\n",
      "\n",
      "Phrase 1: waiter was rude pasta was \n",
      "Category:  FOOD#QUALITY\n",
      "Sentiment:  negative\n",
      "\n",
      "\n",
      "Phrase 2: waiter was rude pasta was nice \n",
      "Category:  FOOD#QUALITY\n",
      "Sentiment:  negative\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Enter a restaurant review (Type 'Q' to Quit):\n",
      "waiter was rude\n",
      "\n",
      "\n",
      "Phrase 1: waiter was rude \n",
      "Category:  SERVICE#GENERAL\n",
      "Sentiment:  negative\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Enter a restaurant review (Type 'Q' to Quit):\n",
      "pasta was tasty\n",
      "\n",
      "\n",
      "Phrase 1: pasta was tasty \n",
      "Category:  FOOD#QUALITY\n",
      "Sentiment:  positive\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Enter a restaurant review (Type 'Q' to Quit):\n",
      "music was good\n",
      "\n",
      "\n",
      "Phrase 1: music was good \n",
      "Category:  RESTAURANT#GENERAL\n",
      "Sentiment:  positive\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d0fb8b31432d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0misUserInput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------------------------------------------------------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0muserInput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter a restaurant review (Type 'Q' to Quit):\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muserInput\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"Q\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;31m# Remove punctuations from sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/FYP/venv/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/FYP/venv/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.tag.stanford import StanfordPOSTagger as POS_Tag\n",
    "from nltk import word_tokenize\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "import re\n",
    "from nltk import sent_tokenize\n",
    "import talos\n",
    "\n",
    "def predictInput(user_input):\n",
    "    # import text and remove those with null category\n",
    "    input_data_category = pd.read_csv(\"ABSACOMBINED.csv\")\n",
    "    input_data_category = input_data_category.dropna(subset=['category'])\n",
    "\n",
    "    input_data_polarity = pd.read_csv(\"ABSACOMBINED.csv\")\n",
    "    input_data_polarity = input_data_polarity.dropna(subset=['polarity'])\n",
    "\n",
    "    # define x_train and y_train data\n",
    "    phrase_category = input_data_category.phrase\n",
    "    category = input_data_category.category\n",
    "\n",
    "    phrase_polarity = input_data_polarity.phrase\n",
    "    polarity = input_data_polarity.polarity\n",
    "\n",
    "    MAX_SEQ_LENGTH = 10  # most of the phrase is within length of 10\n",
    "    MAX_NB_WORDS = 400000  # I set this based on the number of words found in the glove.txt\n",
    "\n",
    "    ############## THIS PART FOR ASPECT CATEGORY ##############\n",
    "    cat_tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=False)\n",
    "    cat_tokenizer.fit_on_texts(phrase_category)\n",
    "    cat_sequences = cat_tokenizer.texts_to_sequences(phrase_category)\n",
    "\n",
    "    category_list = category.tolist()\n",
    "\n",
    "    class_list = ['FOOD#QUALITY', 'SERVICE#GENERAL', 'AMBIENCE#GENERAL', 'RESTAURANT#GENERAL']\n",
    "    filtered_category = []\n",
    "\n",
    "    for i in range(0, 4150):  # to choose the top 4 largest class\n",
    "        if category_list[i] in class_list:\n",
    "            filtered_category.append(category_list[i])\n",
    "\n",
    "    np_category = np.array(filtered_category)\n",
    "\n",
    "    cat_label_encoder = LabelEncoder()\n",
    "    cat_integer_encoded = cat_label_encoder.fit_transform(np_category)\n",
    "\n",
    "    # binary encode\n",
    "    cat_onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    cat_integer_encoded = cat_integer_encoded.reshape(len(cat_integer_encoded), 1)\n",
    "\n",
    "\n",
    "    ############## THIS PART FOR SENTIMENT POLARITY ##############\n",
    "    pol_tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=False)\n",
    "    pol_tokenizer.fit_on_texts(phrase_polarity)\n",
    "    pol_sequences = pol_tokenizer.texts_to_sequences(phrase_polarity)\n",
    "\n",
    "    pol_label_encoder = LabelEncoder()\n",
    "    pol_integer_encoded = pol_label_encoder.fit_transform(polarity)\n",
    "    # binary encode\n",
    "    pol_onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    pol_integer_encoded = pol_integer_encoded.reshape(len(pol_integer_encoded), 1)\n",
    "\n",
    "    ###### THIS PART IS USER INPUT AND OUTPUT #####\n",
    "\n",
    "    # Model already saved, so we can just load the model.\n",
    "    filename_1a = 'finalized_hybrid_lstm_aspect.sav'\n",
    "    filename_1b = 'finalized_hybrid_svm_aspect.sav'\n",
    "    loaded_model_aspect_lstm = joblib.load(filename_1a)\n",
    "    loaded_model_aspect_svm = joblib.load(filename_1b)\n",
    "\n",
    "    filename_2a = 'finalized_hybrid_lstm_sentiment.sav'\n",
    "    filename_2b = 'finalized_hybrid_svm_sentiment.sav'\n",
    "    loaded_model_sentiment_lstm = joblib.load(filename_2a)\n",
    "    loaded_model_sentiment_svm = joblib.load(filename_2b)\n",
    "    \n",
    "    vect = CountVectorizer(max_df=1.0,stop_words='english')\n",
    "\n",
    "    ## User Input Part ##\n",
    "    cat_sequences = cat_tokenizer.texts_to_sequences(user_input)\n",
    "    cat_tokenised_sequence = pad_sequences(cat_sequences, maxlen=MAX_SEQ_LENGTH, padding='post')\n",
    "    cat_np_phrase = np.array(cat_tokenised_sequence)\n",
    "\n",
    "    pol_sequences = pol_tokenizer.texts_to_sequences(user_input)\n",
    "    pol_tokenised_sequence = pad_sequences(pol_sequences, maxlen=MAX_SEQ_LENGTH, padding='post')\n",
    "    pol_np_phrase = np.array(pol_tokenised_sequence)\n",
    "\n",
    "    from keras import backend as K\n",
    "    \n",
    "    outputs = []\n",
    "    for i in range(3, 4):\n",
    "        layer = loaded_model_aspect_lstm.layers[i]\n",
    "        keras_function = K.function([loaded_model_aspect_lstm.input], [layer.output])\n",
    "        outputs.append(keras_function([cat_np_phrase, 1]))\n",
    "    out = np.array(outputs)\n",
    "    r_out = np.reshape(out, (out.shape[2], 10))\n",
    "    predict_aspect = []\n",
    "    for elem in r_out:\n",
    "        e_out = np.expand_dims(elem, axis=0)\n",
    "        predict_aspect.append(loaded_model_aspect_svm.predict(e_out)[0])\n",
    "    sort_list = sorted(class_list)\n",
    "    \n",
    "    outputs = []\n",
    "    for i in range(3, 4):\n",
    "        layer = loaded_model_sentiment_lstm.layers[i]\n",
    "        keras_function = K.function([loaded_model_sentiment_lstm.input], [layer.output])\n",
    "        outputs.append(keras_function([cat_np_phrase, 1]))\n",
    "    out = np.array(outputs)\n",
    "    r_out = np.reshape(out, (out.shape[2], 100))\n",
    "    e_out = np.expand_dims(r_out, axis=0)\n",
    "    #predict_sentiment.append(loaded_model_sentiment_svm.predict(e_out)[0])\n",
    "    predict_sentiment = []\n",
    "    for elem in r_out:\n",
    "        e_out = np.expand_dims(elem, axis=0)\n",
    "        predict_sentiment.append(loaded_model_sentiment_svm.predict(e_out)[0])\n",
    "    \n",
    "    sort_list = sorted(class_list)\n",
    "    polarity_list = ['negative', 'neutral', 'positive']\n",
    "    for i in range(len(user_input)):\n",
    "        print(\"\\n\")\n",
    "        print(\"Phrase \" + str(i+1) + \":\", user_input[i])\n",
    "        print(\"Category: \", sort_list[predict_aspect[i]])\n",
    "        print(\"Sentiment: \", polarity_list[predict_sentiment[i]])\n",
    "\n",
    "\n",
    "def getAspectInput(tagged_review):\n",
    "    aspectlist=[]\n",
    "    for text_list in tagged_review:\n",
    "        for word,tag in text_list:\n",
    "            if tag in ['NN','NNS','NNP','NNPS']:\n",
    "                aspectlist.append(word)\n",
    "    return aspectlist\n",
    "\n",
    "def get_all_phrases_containing_tar_wrd(target_word, tar_passage, left_margin=5, right_margin=5):\n",
    "    \"\"\"\n",
    "        Function to get all the pharses that contain the target word in a text/passage tar_passage.\n",
    "        Workaround to save the output given by nltk Concordance function\n",
    "\n",
    "        str target_word --> aspect to be searched for\n",
    "        str tar_passage  --> sentence extracted from a customer review\n",
    "        int left_margin int right_margin --> left_margin and right_margin allocate the number of words/punctuation before and after target word\n",
    "        Left margin will take note of the beginning of the text\n",
    "    \"\"\"\n",
    "\n",
    "    ## Create list of tokens using nltk function\n",
    "    tokens = nltk.word_tokenize(tar_passage)\n",
    "    tokens = [x for x in tokens if len(x) > 2]\n",
    "    ## Create the text of tokens\n",
    "    text = nltk.Text(tokens)\n",
    "\n",
    "    ## Collect all the index or offset position of the target word\n",
    "    c = nltk.ConcordanceIndex(text.tokens, key=lambda s: s.lower())\n",
    "\n",
    "    ## Collect the range of the words that is within the target word by using text.tokens[start;end].\n",
    "    ## The map function is use so that when the offset position - the target range < 0, it will be default to zero\n",
    "\n",
    "    concordance_txt = (\n",
    "    [text.tokens[list(map(lambda x: x - 5 if (x - left_margin) > 0 else 0, [offset]))[0]:offset + right_margin] for\n",
    "     offset in c.offsets(target_word)])\n",
    "\n",
    "    ## join the sentences for each of the target phrase and return it\n",
    "    return [''.join([x + ' ' for x in con_sub]) for con_sub in concordance_txt]\n",
    "\n",
    "# java_path = \"C:/Program Files/Java/jdk-11.0.2/bin/java.exe\" - Wei Ming's Java path\n",
    "java_path = \"C:/Program Files/Java/jdk1.8.0_201/bin/java.exe\"\n",
    "os.environ['JAVA_HOME'] = java_path\n",
    "\n",
    "#For stanford POS Tagger\n",
    "home = os.getcwd() + \"/stanford-postagger-2018-10-16\"\n",
    "_path_to_model = home + '/models/english-bidirectional-distsim.tagger'\n",
    "_path_to_jar = home + '/stanford-postagger.jar'\n",
    "stanford_tag = POS_Tag(model_filename=_path_to_model, path_to_jar=_path_to_jar)\n",
    "\n",
    "#To tag using stanford pos tagger\n",
    "def posTag(review):\n",
    "    tagged_text_list=[]\n",
    "    for text in review:\n",
    "        tagged_text_list.append(stanford_tag.tag(word_tokenize(text)))\n",
    "    return tagged_text_list\n",
    "\n",
    "#Filter the word with tag- noun,adjective,verb,adverb\n",
    "def filterTag(tagged_review):\n",
    "    final_text_list=[]\n",
    "    for text_list in tagged_review:\n",
    "        final_text=[]\n",
    "        for word,tag in text_list:\n",
    "            if tag in ['NN','NNS','NNP','NNPS','RB','RBR','RBS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ']:\n",
    "                final_text.append(word)\n",
    "        final_text_list.append(' '.join(final_text))\n",
    "    return final_text_list\n",
    "\n",
    "\n",
    "# Reduce duplicated letters in a word to be maximum of 2.\n",
    "\n",
    "def word_lengthening(sentence):\n",
    "    list_words = sentence\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    reduced_list = [pattern.sub(r\"\\1\\1\", word) for word in list_words] # Perform reduce lenghtening\n",
    "    return reduced_list\n",
    "\n",
    "# Perform spell correction\n",
    "# Downside: Some names/abbreviations are also used for spell correction which could cause some inconsistency.\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "def spell_correction(sentence):\n",
    "    list_words = sentence\n",
    "    spell_list = [spell.correction(word) for word in list_words]\n",
    "    return spell_list\n",
    "\n",
    "# Remove punctuations from all sentences\n",
    "def remove_punct(my_sentence):\n",
    "    trans_table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    no_punct = my_sentence.translate(trans_table)\n",
    "    return no_punct\n",
    "\n",
    "isUserInput = True\n",
    "while isUserInput:\n",
    "    print(\"--------------------------------------------------------------------------------------------------\")\n",
    "    userInput = input(\"Enter a restaurant review (Type 'Q' to Quit):\\n\")\n",
    "    if userInput != \"Q\":\n",
    "        # Remove punctuations from sentence\n",
    "        userInput = userInput.lower()\n",
    "        userInput = remove_punct(userInput)\n",
    "\n",
    "        # Tokenize input sentence\n",
    "        token_input = nltk.word_tokenize(userInput)\n",
    "        # print(\"Tokenize words: \", token_input)\n",
    "\n",
    "        # Perform word correction\n",
    "        word_correction = word_lengthening(token_input)\n",
    "        correct_sentence = spell_correction(word_correction)\n",
    "        joined_words = (\" \".join(correct_sentence))\n",
    "\n",
    "        # Preprocessing and vectorizing\n",
    "        tagged_user_input = posTag([joined_words])\n",
    "        # print(\"Part-of-Speech Tagging: \", tagged_user_input)\n",
    "        filter_tagged_user_input = filterTag(tagged_user_input)\n",
    "        # print(\"Filtered Part-of-Speech Tagging: \", filter_tagged_user_input[0])\n",
    "        aspect_list = getAspectInput(tagged_user_input)\n",
    "\n",
    "        phrases=[]\n",
    "\n",
    "        for sentence in sent_tokenize(filter_tagged_user_input[0]):\n",
    "            for important_word in aspect_list:\n",
    "                phrases_in_sentence = get_all_phrases_containing_tar_wrd(important_word, sentence, left_margin = 5, right_margin = 5)\n",
    "                for phrase in phrases_in_sentence:\n",
    "                    phrases.append(phrase)\n",
    "\n",
    "        phrases = list(set(phrases))\n",
    "        predictInput(phrases)\n",
    "    else:\n",
    "        isUserInput = False\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
