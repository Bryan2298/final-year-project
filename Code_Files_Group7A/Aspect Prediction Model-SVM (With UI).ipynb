{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing NaN value:  (4579, 5)\n",
      "After removing NaN value:  (4150, 5)\n",
      "      review_id                                           category\n",
      "0             0                                 RESTAURANT#GENERAL\n",
      "1             1                    SERVICE#GENERAL,SERVICE#GENERAL\n",
      "2             2  SERVICE#GENERAL,SERVICE#GENERAL,SERVICE#GENERA...\n",
      "3             3                          FOOD#QUALITY,FOOD#QUALITY\n",
      "4             4              FOOD#STYLE_OPTIONS,FOOD#STYLE_OPTIONS\n",
      "...         ...                                                ...\n",
      "2151       3155              FOOD#STYLE_OPTIONS,FOOD#STYLE_OPTIONS\n",
      "2152       3157                                       FOOD#QUALITY\n",
      "2153       3158                                     DRINKS#QUALITY\n",
      "2154       3159                                   AMBIENCE#GENERAL\n",
      "2155       3160                                  RESTAURANT#PRICES\n",
      "\n",
      "[2156 rows x 2 columns]\n",
      "FOOD#QUALITY                1368\n",
      "SERVICE#GENERAL              747\n",
      "AMBIENCE#GENERAL             483\n",
      "RESTAURANT#GENERAL           440\n",
      "FOOD#STYLE_OPTIONS           314\n",
      "DRINKS#QUALITY               163\n",
      "FOOD#PRICES                  157\n",
      "RESTAURANT#MISCELLANEOUS     144\n",
      "RESTAURANT#PRICES            129\n",
      "DRINKS#STYLE_OPTIONS          80\n",
      "LOCATION#GENERAL              78\n",
      "DRINKS#PRICES                 45\n",
      "FOOD#GENERAL                   2\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# import text and remove those with null category\n",
    "input_data = pd.read_csv(\"ABSACOMBINED.csv\")\n",
    "print('Before removing NaN value: ', input_data.shape)\n",
    "input_data = input_data.dropna(subset=['category'])\n",
    "print('After removing NaN value: ', input_data.shape)\n",
    "\n",
    "# define data grouped by review id (as dataframe)\n",
    "grouped_df = input_data.groupby('review_id')\n",
    "grouped_data = grouped_df['category'].agg(lambda column: \",\".join(column))\n",
    "grouped_data = grouped_data.reset_index(name='category')\n",
    "print(grouped_data)\n",
    "\n",
    "# define x_train and y_train data\n",
    "review_id = input_data.review_id\n",
    "phrase = input_data.phrase\n",
    "category = input_data.category\n",
    "print(category.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 10  # most of the phrase is within length of 10\n",
    "MAX_NB_WORDS = 400000  # I set this based on the number of words found in the glove.txt (should have no effect as only 2744 tokens found below) \n",
    "EMBEDDING_DIM = 100  # I tried using glove 100d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vectorize the text samples into a 2D integer tensor and padding the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2744 unique tokens.\n",
      "Let's have a quick look at the word_index data..\n",
      "[('food', 1), ('great', 2), ('good', 3), ('service', 4), ('place', 5), ('nice', 6), ('best', 7), ('excellent', 8), ('restaurant', 9), ('menu', 10)]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=False)\n",
    "tokenizer.fit_on_texts(phrase)\n",
    "sequences = tokenizer.texts_to_sequences(phrase)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print (\"Let's have a quick look at the word_index data..\")\n",
    "print (list(word_index.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1946 1211 1947 ...    0    0    0]\n",
      " [ 486  899 1456 ...   12 1948  199]\n",
      " [ 486  899 1456 ...   12    0    0]\n",
      " ...\n",
      " [   3    1   58 ...    0    0    0]\n",
      " [   3    1   58 ...    0    0    0]\n",
      " [   3    1   58 ...    0    0    0]]\n",
      "Shape of data tensor: (4150, 10)\n"
     ]
    }
   ],
   "source": [
    "tokenised_sequence = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post')\n",
    "print(tokenised_sequence)\n",
    "print('Shape of data tensor:', tokenised_sequence.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### defining output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_id_list = review_id.tolist()\n",
    "category_list = category.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AMBIENCE#GENERAL', 483), ('FOOD#QUALITY', 1368), ('RESTAURANT#GENERAL', 440), ('SERVICE#GENERAL', 747)]\n",
      "[1.57246377 0.55519006 1.72613636 1.0167336 ]\n"
     ]
    }
   ],
   "source": [
    "# from imblearn.over_sampling import (RandomOverSampler, \n",
    "#                                     SMOTE, \n",
    "#                                     ADASYN)\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_list = ['FOOD#QUALITY', 'SERVICE#GENERAL', 'AMBIENCE#GENERAL', 'RESTAURANT#GENERAL']\n",
    "filtered_phrase = []\n",
    "filtered_category = []\n",
    "filtered_id = []\n",
    "\n",
    "for i in range(0, 4150):  # to choose the top 4 largest class\n",
    "    if category_list[i] in class_list:\n",
    "        filtered_id.append(review_id_list[i])\n",
    "        filtered_phrase.append(tokenised_sequence[i])\n",
    "        filtered_category.append(category_list[i])\n",
    "        \n",
    "np_phrase = np.array(filtered_phrase)  # So this is without oversampling, the accuracy improves from 0.2 to 0.4 but still all same prob\n",
    "np_category = np.array(filtered_category)\n",
    "print(sorted(Counter(np_category).items()))\n",
    "\n",
    "# smote_x, smote_y = SMOTE().fit_sample(filtered_x, filtered_y)\n",
    "\n",
    "# class weight to handle imbalanced data\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(np_category),\n",
    "                                                 np_category)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transform output data into categorical index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "(3038, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fx505\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(np_category)\n",
    "    \n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "print(onehot_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tidy up all variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% as training data\n",
    "# 20% as testing data\n",
    "x_train = np_phrase[:2431]\n",
    "x_test = np_phrase[2431:3039]\n",
    "y_train = onehot_encoded[:2431]\n",
    "y_test = onehot_encoded[2431:3039]\n",
    "id_test = filtered_id[2431:3039]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5189456342668863\n"
     ]
    }
   ],
   "source": [
    "svm = make_pipeline(StandardScaler(), SVC(gamma=\"auto\"))\n",
    "svm.fit(x_train, np.argmax(y_train, axis = 1))\n",
    "res = svm.predict(x_test)\n",
    "print(svm.score(x_test, np.argmax(y_test, axis = 1), sample_weight=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_res = label_encoder.inverse_transform(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_id\n",
       "2400    RESTAURANT#GENERAL\n",
       "2403          FOOD#QUALITY\n",
       "2404          FOOD#QUALITY\n",
       "2407          FOOD#QUALITY\n",
       "2409          FOOD#QUALITY\n",
       "               ...        \n",
       "3150          FOOD#QUALITY\n",
       "3152          FOOD#QUALITY\n",
       "3153          FOOD#QUALITY\n",
       "3157          FOOD#QUALITY\n",
       "3159          FOOD#QUALITY\n",
       "Name: predicted_category, Length: 366, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'review_id': id_test,\n",
    "     'predicted_category': pre_res})\n",
    "predicted_data = df.groupby('review_id')['predicted_category'].apply(lambda x: ','.join(x))\n",
    "predicted_data  # this is in series format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fx505\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "grouped_data.to_csv(\"svm_actual_category.csv\")\n",
    "predicted_data.to_csv(\"svm_predicted_category.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5519125683060109\n"
     ]
    }
   ],
   "source": [
    "no_of_sentence = predicted_data.size\n",
    "no_of_sentence_correct = 0\n",
    "for index, value in predicted_data.items():\n",
    "    match = True\n",
    "    predicted_value = value.split(',')\n",
    "    actual_value = grouped_data[grouped_data['review_id'] == index]['category']\n",
    "    actual_value = actual_value.tolist()[0].split(',')\n",
    "    for elem in actual_value:\n",
    "        if elem not in predicted_value:\n",
    "            match = False\n",
    "            break\n",
    "    if match:\n",
    "        no_of_sentence_correct += 1\n",
    "\n",
    "print(no_of_sentence_correct/no_of_sentence)\n",
    "\n",
    "# END HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(max_df=1.0,stop_words='english')  \n",
    "\n",
    "# java_path = \"C:/Program Files/Java/jdk-11.0.2/bin/java.exe\" - Wei Ming's Java path\n",
    "java_path = \"C:/Program Files/Java/jdk1.8.0_201/bin/java.exe\"\n",
    "os.environ['JAVA_HOME'] = java_path \n",
    "\n",
    "#For stanford POS Tagger\n",
    "home = os.getcwd() + \"/stanford-postagger-2018-10-16\"\n",
    "from nltk.tag.stanford import StanfordPOSTagger as POS_Tag\n",
    "from nltk import word_tokenize\n",
    "_path_to_model = home + '/models/english-bidirectional-distsim.tagger' \n",
    "_path_to_jar = home + '/stanford-postagger.jar'\n",
    "stanford_tag = POS_Tag(model_filename=_path_to_model, path_to_jar=_path_to_jar)\n",
    "\n",
    "#To tag using stanford pos tagger\n",
    "def posTag(review):\n",
    "    tagged_text_list=[]\n",
    "    for text in review:\n",
    "        tagged_text_list.append(stanford_tag.tag(word_tokenize(text)))\n",
    "    return tagged_text_list\n",
    "\n",
    "#Filter the word with tag- noun,adjective,verb,adverb\n",
    "def filterTag(tagged_review):\n",
    "    final_text_list=[]\n",
    "    for text_list in tagged_review:\n",
    "        final_text=[]\n",
    "        for word,tag in text_list:\n",
    "            if tag in ['NN','NNS','NNP','NNPS','RB','RBR','RBS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ']:\n",
    "                final_text.append(word)\n",
    "        final_text_list.append(' '.join(final_text))\n",
    "    return final_text_list\n",
    "\n",
    "\n",
    "# Reduce duplicated letters in a word to be maximum of 2.\n",
    "\n",
    "def word_lengthening(sentence):\n",
    "    list_words = sentence\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    reduced_list = [pattern.sub(r\"\\1\\1\", word) for word in list_words] # Perform reduce lenghtening\n",
    "    return reduced_list\n",
    "\n",
    "# Perform spell correction\n",
    "# Downside: Some names/abbreviations are also used for spell correction which could cause some inconsistency.\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "def spell_correction(sentence):\n",
    "    list_words = sentence\n",
    "    spell_list = [spell.correction(word) for word in list_words]\n",
    "    return spell_list\n",
    "\n",
    "# Remove punctuations from all sentences\n",
    "def remove_punct(my_sentence):\n",
    "    trans_table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    no_punct = my_sentence.translate(trans_table)\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a restaurant review:\n",
      "\n",
      "I loove this restaurant\n",
      "Tokenize words:  ['I', 'loove', 'this', 'restaurant']\n",
      "Spelling correction:  I love this restaurant\n",
      "Part-of-Speech Tagging:  [[('I', 'PRP'), ('love', 'VBP'), ('this', 'DT'), ('restaurant', 'NN')]]\n",
      "Filtered Part-of-Speech Tagging:  ['love restaurant']\n",
      "Category:  [1]\n"
     ]
    }
   ],
   "source": [
    "#Aspect analyis of user's input.\n",
    "user_input=input(\"Enter a restaurant review:\\n\\n\")\n",
    "\n",
    "# Remove punctuations from sentence\n",
    "user_input = remove_punct(user_input)\n",
    "\n",
    "# Tokenize input sentence\n",
    "token_input = nltk.word_tokenize(user_input)\n",
    "print(\"Tokenize words: \", token_input)\n",
    "\n",
    "# Perform word correction\n",
    "word_correction = word_lengthening(token_input)\n",
    "correct_sentence = spell_correction(word_correction)\n",
    "joined_words = ( \" \".join(correct_sentence))\n",
    "print(\"Spelling correction: \", joined_words)\n",
    "\n",
    "#Preprocessing and vectorizing\n",
    "tagged_user_input = posTag([joined_words])\n",
    "print(\"Part-of-Speech Tagging: \", tagged_user_input)\n",
    "filter_tagged_user_input = filterTag(tagged_user_input)\n",
    "print(\"Filtered Part-of-Speech Tagging: \", filter_tagged_user_input)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(filter_tagged_user_input)\n",
    "tokenised_sequence = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post')\n",
    "np_phrase = np.array(tokenised_sequence)\n",
    "\n",
    "predict_aspect= svm.predict(np_phrase)\n",
    "print(\"Category: \", predict_aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
